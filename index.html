<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Hyperspace by HTML5 UP</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Sidebar -->
			<section id="sidebar">
				<div class="inner">
					<nav>
						<ul>
							<li><a href="#intro">Introduction</a></li>
							<li><a href="#one">Data</a></li>
							<li><a href="#two">Clustering</a></li>
							<li><a href="#three">ARM</a></li>
							<li><a href="#four">LDA</a></li>
						</ul>
					</nav>
				</div>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Intro -->
					<section id="intro" class="wrapper style1 fullscreen fade-up">
						<div class="inner">
							<h1>Will ChatGPT replace Google ?</h1>
							<p>Since OpenAI released ChatGPT, there has been a lot of speculation about what its killer app will be. And perhaps topping the list is online search. According to The New York Times, Google’s management has declared a “code red” and is scrambling to protect its online search monopoly against the disruption that ChatGPT will bring.
								<br/><br/> First, let's start by understanding what ChatGPT and Google are respectively.<br/>
								<h1>Google</h1>
Let’s begin with Google. Google is an internet search engine. It uses a proprietary algorithm that's designed to retrieve and order search results to provide the most relevant and dependable sources of data possible.
Google's stated mission is to "organize the world's information and make it universally accessible and useful." It is the top search engine in the world, a position that has generated criticism and concern about the power it has to influence the flow of online information.
<br/><br/><h1>ChatGPT</h1>ChatGPT is, in essence, a simple online artificial intelligence chatbot created by OpenAI in December 2022.ChatGPT is an AI language model developed by OpenAI, which is capable of generating human-like text based on the input it is given. The model is trained on a large corpus of text data and can generate responses to questions, summarize long texts, write stories and much more. It is often used in conversational AI applications to simulate a human-like conversation with users.You can use ChatGPT to message people on dating apps; to write essays; to create malware (yikes); and more.
							<br/><br/><h1>So how is ChatGPT different from Google ?</h1>Let's explore the based on the following :<br/><br/>
						<ul>
							<li><h2>Up-To-Date Information</h2></li><p>Chat GPT is not up to date on information. The data in GPT is trained up to May 2020. Therefore, ChatGPT has no understanding of the world after May 2020. It goes without saying, Google is pretty up-to-date since I use it to check the weather, news, and GPS. As of today, it also cost between 10–20 million dollars in computing (server) costs and 1–3 months in time to train (teach) the ChatGPT algorithm this information. As you can see, that’s not sustainable for consistently updating data.</p>
							<li><h2>Completeness Of Information</h2></li><p>ChatGPT only contains a fraction of the information indexed by Google. It uses a dataset called WebCrawl which contains about 75% of the information on the internet. Search engine data is constantly updated and, therefore relevant. People also actively submit information to search engines. In the future, this may change as AI capabilities advance, but ChatGPT will unlikely ever become as up-to-date and complete as a search engine.</p>
							<li><h2>Accuracy of Information</h2></li><p>The information on ChatGPT is not always accurate. It is a synthesized best guess based on a large amount of data. Sometimes it misunderstands information similar to humans who concoct stories for semi-related topics( Ex: Lizard People). Search engines also do not always present accurate articles; however, they provide a large collection of content that the user can sort through and fact-check. There is no way to fact-check ChatGPT results since the sources are not listed!</p>
							<li><h2>Information Format</h2></li><p>The information in Chat GPT is shown to the user as paragraphs. It is a summarized version of the data that exist on the topic. Information in a search engine is presented back as lists or tables. It provides links to articles and resources that might answer your question.</p>
						</ul>
					<br/>
					This is likely where the crux of the argument for why Chat GPT is better than Google or whether it will replace Google exists. Chat GPT provides information that is easier and quicker to consume. However, for the reasons stated above, it fails to serve the purpose of a search engine for deeper questions that require accuracy. Chat GPT can not find you a doctor, it can not provide dating advice, it can not show current news events, and it is not 100% accurate. In the world of fake news, accuracy matters, and being able to determine what are real facts and what are opinions is a crucial task for human decision-making. So, will it replace Google or be integrated and inspire it to become better ? Let's see what the data we gathered from various discussion forums would tell us !
					</p>
						</div>
					</section>

				<!-- One -->
					<section id="one" class="wrapper style2 spotlights">
						<section>
							<div class="content">
								<div class="inner">
									<h2>Data Collection and Methodology</h2>
									<p>For the purpose of this project, I decided to gather data from Reddit API, NewsAPI and scraped Medium(blogging site). The links to all the scrapers :
										<ul>
											<li><a href="https://colab.research.google.com/drive/1zds8CocXlKJGBiksuOrJiRkdg_ZSBtT2?usp=sharing" target="_blank">Reddit Scraper</a></li>
											<li><a href="https://colab.research.google.com/drive/1e7svi2NBulK0VTQz99T_BSJ9gJ7JB5tI?usp=sharing" target="_blank">Medium Scraper</a></li>
											<li><a href="https://colab.research.google.com/drive/190QGPw6tooSHIDhLt_AN8vh4KVwCb0eA?usp=sharing" target="_blank">NewsAPI Scaper</a></li>
										</ul>
										For scraping Reddit,Reddit API was used and with the help of the PRAW library, data was scraped out from the following subreddits :
										<ul>
											<li>singularity</li>
											<li>google</li>
											<li>Futurology</li>
											<li>technology</li>
											<li>ChatGPT</li>
											<li>artificial</li>
											<li>ArtificialInteligence</li>
											<li>Machinelearningnews</li>
											<li>MachineLearning</li>
											<li>datascience</li>
											<li>MLQuestions</li>
										</ul> 
										The query used was : 'Will ChatGPT replace Google ?'. 
										<br/><br/>For NewsAPI, the end point used was /everything. 
										<br/><br/>For Medium the <a href="https://medium.com/search?q=will+chatGPT+replace+Google+search" target="_blank">link</a> was used to scrape the data from the articles/blogs that showed up after the search.
									    <br/><br/> 
									</p>
									
								</div>
							</div>
						</section>
						<section>
							<div class="content">
								<div class="inner">
									<h2>Data before cleaning</h2>
									<p>The data from all these sites was gathered in the form of csv files and was obtained as separate files. Then, a sample of content was collected concerning the topic at hand to arrive at the file that was 
										finally used for the analysis. After that the final .csv file which is being used for analysis was labelled manually. The following four labels are being used to describe the views addressed about whether 
									chatGPT will replace Google or not ?
									<ul>
										<li>Yes</li>
										<li>No</li>
										<li>Neutral</li>
										<li>Maybe</li>
									</ul> 
									Snapshot of data before cleaning :<br/><br/>
									<img src="images/Unclean.png" alt="Unclean" width="400" height="400"/>

									<br/>
									Link to all the <a href="https://github.com/Nikhil49Kr/Nikhil49Kr.github.io/tree/main/Data" target="_blank">data</a> files.
 								</p>
									
								</div>
							</div>
						</section>
						<section>
							<div class="content">
								<div class="inner">
									<h2>Data after cleaning</h2>
									<p>As expected, the data gathered can't be directly used for modeling purpose. It has to be cleaned.
										Cleaning text-data is a typical pre-processing task for data science and machine learning. It consists of getting rid of the less useful parts of text through stopword removal, dealing with capitalization, special characters and other details.Specifically, these steps were taken to do the same here :
										<ul>
											<li>Sneak peek into the data</li>
											<li>Whitespace/Punctuation/Normalize Case</li>
											<li>Stopwords removal</li>
											<li>Stemming</li>
											<li>Lemmatization</li>
											<li>Link removal</li>
											<li>Vectorization using CountVectorizer and TfidfVectorizer</li>
										</ul>
									Snapshot of data after cleaning : <br/><br/>
									<img src="images/Clean.png" alt="Clean" width="400" height="400"/>
									<br/>
									Script used for data procession and cleaning : <a href="https://colab.research.google.com/drive/16FyHdkw5OkLxCgIExuys4CiYoT7Okxwz?usp=sharing" target="_blank">Data_Cleaning.ipynb</a>	
									</p>
							
								</div>
							</div>
						</section>
					</section>
				
				<!-- Two -->
				<section id="two" class="wrapper style2 spotlights">
					<section>
						<div class="content">
							<div class="inner">
								<h2>Clustering</h2>
								<p>Clustering is a machine learning technique used to group similar objects together in a dataset based on their inherent similarities. The objective of clustering is to partition a given dataset into several groups, or clusters, such that objects in the same group are more similar to each other than to those in other groups. The process of clustering involves assigning each data point to a cluster based on some similarity metric. There are several algorithms used in clustering, including k-means, hierarchical clustering, and density-based clustering. Clustering is used in various applications such as customer segmentation, image segmentation, anomaly detection, and recommender systems. For example, clustering can be used to group customers with similar behavior together in order to personalize marketing campaigns or to identify fraudulent behavior based on anomalies in a dataset. 
								</p>
								
							</div>
						</div>
					</section>
					<section>
						<div class="content">
							<div class="inner">
								<h2>Overview</h2>
								<p>The data for this project was gathered by making API calls or web scraping which meant the data gathered came without labels. Hence, the
									clustering methods can be used to convert this data without labels into data with labels by clustering the same into similar groups. The methods used 
									to do the same here are :
								<ul>
									<li>K Means</li>
									<li>Hierarchical Clustering</li>
								</ul> 
								By using the methods above, I intend to find the optimal number of clusters in my unlabeled data in a more automated way.
							 </p>
								
							</div>
						</div>
					</section>
					<section>
						<div class="content">
							<div class="inner">
								<h2>Data Prep</h2>
								<p>As expected, the data gathered can't be directly used for modeling purpose. It has to be cleaned.
									Cleaning text-data is a typical pre-processing task for data science and machine learning. It consists of getting rid of the less useful parts of text through stopword removal, dealing with capitalization, special characters and other details.Specifically, these steps were taken to do the same here :
									<ul>
										<li>Sneak peek into the data</li>
										<li>Whitespace/Punctuation/Normalize Case</li>
										<li>Stopwords removal</li>
										<li>Stemming</li>
										<li>Lemmatization</li>
										<li>Link removal</li>
										<li>Vectorization using CountVectorizer and TfidfVectorizer</li>
									</ul>
								Snapshot of data after cleaning : <br/><br/>
								<img src="images/Clean.png" alt="Clean" width="400" height="400"/>
								<br/>
								Script used for data procession and cleaning : <a href="https://colab.research.google.com/drive/16FyHdkw5OkLxCgIExuys4CiYoT7Okxwz?usp=sharing" target="_blank">Data_Cleaning.ipynb</a>	
								</p>
						
							</div>
						</div>
					</section>

					<section>
						<div class="content">
							<div class="inner">
								<h2>Code</h2>
								<p>
								Script used for Kmeans Clustering : <a href="https://colab.research.google.com/drive/16FyHdkw5OkLxCgIExuys4CiYoT7Okxwz?usp=sharing" target="_blank">KMeans.ipynb</a>
								<br/>
								Script used for Hierarchical Clustering : <a href="https://github.com/Nikhil49Kr/Nikhil49Kr.github.io/blob/main/Hierarchical_Clustering.R" target="_blank">Hierarchical_Clustering.R</a>	
								</p>
						
							</div>
						</div>
					</section>

					<section>
						<div class="content">
							<div class="inner">
								<h2>Results</h2>
								<p>Elbow method plot :
									<br/>
								<img src="images/Elbow.png" alt="Clean" width="400" height="400"/>
								<br/>
									Silhoutte method plot : <br/><img src="images/Sil.png" alt="Clean" width="400" height="400"/>
								<br/><br/>
								As can be seen from the above , 4 seems to be value of k which seems to be optimal.	
								<br/><br/>
								Dendogram from Hierarchical Clustering :<br/><br/>
								<img src="images/hierclus.png" alt="Clean" width="600" height="500"/>
								</p>
							</div>
						</div>
					</section>

					<section>
		
						<div class="content">
							<div class="inner">
								<h2>Conclusion</h2>
								<p>
									Hence, the data aggregated from various sources for this project can be divided into four separate categories.
									<br/><br/>
									<h3>Top words per cluster :</h3>
									<br/>
									<h4>Cluster 0 :</h4>
									<ul>
										<li>ai</li>
										<li>like</li>
										<li>think</li>
										<li>time</li>
										<li>work</li>
										<li>job</li>
										<li>know</li>
										<li>good</li>
										<li>need</li>
										<li>cover</li>
									</ul>
									<h4>Cluster 1 :</h4>
									<ul>
										<li>google</li>
										<li>search</li>
										<li>chatgpt</li>
										<li>ai</li>
										<li>like</li>
										<li>results</li>
										<li>engine</li>
										<li>use</li>
										<li>better</li>
										<li>think</li>
									</ul>
									<h4>Cluster 2 :</h4>
									<ul>
										<li>chatgpt</li>
										<li>like</li>
										<li>write</li>
										<li>better</li>
										<li>know</li>
										<li>use</li>
										<li>job</li>
										<li>code</li>
										<li>using</li>
										<li>time</li>
									</ul>
									<h4>Cluster 3 :</h4>
									<ul>
										<li>people</li>
										<li>jobs</li>
										<li>ai</li>
										<li>work</li>
										<li>like</li>
										<li>going</li>
										<li>think</li>
										<li>job</li>
										<li>replace</li>
										<li>new</li>
									</ul>

								</p>
						
							</div>
						</div>
					</section>
				</section>
			
			<!-- Three -->
			<section id="three" class="wrapper style2 spotlights">
				<section>
					<div class="content">
						<div class="inner">
							<h2>ARM</h2>
							<p>Association rule mining is a data mining technique that involves finding relationships or associations between variables in a large dataset. Specifically, it aims to discover patterns and relationships among the items in a dataset, based on how frequently they occur together. 
								The main goal of association rule mining is to identify the underlying patterns and rules that govern the behavior of a particular dataset. This can be useful in a variety of real-world applications, such as :
								<ul>
									<li>Market Basket Analysis</li>
									<li>Fraud Detection</li>
									<li>Web Usage Analysis</li>
									<li>Social Network Analysis</li>
								</ul>
								Overall, association rule mining is a powerful tool for discovering hidden patterns and relationships in large datasets, which can be used to make informed decisions and drive business value in a variety of real-world applications. 
							</p>
							
						</div>
					</div>
				</section>
				<section>
					<div class="content">
						<div class="inner">
							<h2>Overview</h2>
							<p>
								While ARM can be useful for analyzing and understanding complex relationships between variables, it may not be the best approach to answer questions like "Will ChatGPT replace Google."
								To answer this question, we need to consider various factors such as the current market share, user adoption, technological advancements, and competitive landscape. 
								ARM may not be able to capture all of these factors, and the patterns or relationships it identifies may not provide a definitive answer to the question.That being said, 
								since we have a dataset containing responses to the question, we can still use ARM to identify interesting associations or patterns in the data. For example, you could use ARM to identify frequent co-occurrences of keywords or phrases related to ChatGPT and Google in the responses.
							</p>
							
						</div>
					</div>
				</section>
				<section>
					<div class="content">
						<div class="inner">
							<h2>Data Prep</h2>
							<p>As expected, the data gathered can't be directly used for modeling purpose. It has to be cleaned.
								Cleaning text-data is a typical pre-processing task for data science and machine learning. It consists of getting rid of the less useful parts of text through stopword removal, dealing with capitalization, special characters and other details.Specifically, these steps were taken to do the same here :
								<ul>
									<li>Sneak peek into the data</li>
									<li>Whitespace/Punctuation/Normalize Case</li>
									<li>Stopwords removal</li>
									<li>Stemming</li>
									<li>Lemmatization</li>
									<li>Link removal</li>
									<li>Vectorization using CountVectorizer and TfidfVectorizer</li>
								</ul>
							Snapshot of data after cleaning : <br/><br/>
							<img src="images/Clean.png" alt="Clean" width="400" height="400"/>
							<br/>
							Script used for data procession and cleaning : <a href="https://colab.research.google.com/drive/16FyHdkw5OkLxCgIExuys4CiYoT7Okxwz?usp=sharing" target="_blank">Data_Cleaning.ipynb</a>	
							</p>
					
						</div>
					</div>
				</section>

				<section>
					<div class="content">
						<div class="inner">
							<h2>Code</h2>
							<p>
							Script used for ARM : <a href="https://github.com/Nikhil49Kr/Nikhil49Kr.github.io/blob/main/ARM.R" target="_blank">ARM.R</a>
							</p>
					
						</div>
					</div>
				</section>

				<section>
					<div class="content">
						<div class="inner">
							<h2>Results</h2>
							<p>Top 10 rules generated by ARM : <br/>
								<img src="images/Top10_ARM.png" alt="Clean" width="400" height="400"/>
								<br/>
								Plot :<br/>
								<img src="images/ARM.png" alt="Clean" width="400" height="400"/>
							</p>
					
						</div>
					</div>
				</section>

				<section>
					<div class="content">
						<div class="inner">
							<h2>Conclusion</h2>
							<p>
								In conclusion, we used ARM to identify frequent co-occurrences of keywords or phrases related to ChatGPT and Google in the responses. However, 
								it may not be the best approach to answer questions like "Will ChatGPT replace Google." To answer this question, we need to consider various factors such as the current market share, user adoption, technological advancements, and competitive landscape.
							</p>
					
						</div>
					</div>
				</section>
			</section>

			<!-- Four -->
			<section id="four" class="wrapper style2 spotlights">
				<section>
					<div class="content">
						<div class="inner">
							<h2>Topic Modeling</h2>
							<p>Topic modeling is a statistical technique that involves identifying topics or themes that are present in a collection of documents. It is often used in natural language processing and text mining applications to uncover the underlying structure of a large corpus of text.

								At a high level, topic modeling works by analyzing the frequency of words and phrases in a collection of documents and identifying groups of words that tend to appear together. These groups of words are known as "topics," and they can be used to summarize the content of the documents and identify common themes or patterns.
								
								Topic modeling has a wide range of applications in the real world, including :
								<ul>
									<li>Text Classification</li>
									<li>Recommender Systems</li>
									<li>Sentiment Analysis</li>
									<li>Market Research</li>
									<li>Trend Analysis</li>
								</ul>
								Overall, topic modeling is a powerful tool for uncovering the underlying structure and themes in large collections of text, which can be used to inform a wide range of real-world applications. 
							</p>
							
						</div>
					</div>
				</section>
				<section>
					<div class="content">
						<div class="inner">
							<h2>Overview</h2>
							<p>
								Out of all the use cases mentioned above, I intend to use Topic Modeling for the purpose of Text Classification in this project. 
							</p>
						</div>
					</div>
				</section>
				<section>
					<div class="content">
						<div class="inner">
							<h2>Data Prep</h2>
							<p>As expected, the data gathered can't be directly used for modeling purpose. It has to be cleaned.
								Cleaning text-data is a typical pre-processing task for data science and machine learning. It consists of getting rid of the less useful parts of text through stopword removal, dealing with capitalization, special characters and other details.Specifically, these steps were taken to do the same here :
								<ul>
									<li>Sneak peek into the data</li>
									<li>Whitespace/Punctuation/Normalize Case</li>
									<li>Stopwords removal</li>
									<li>Stemming</li>
									<li>Lemmatization</li>
									<li>Link removal</li>
									<li>Vectorization using CountVectorizer and TfidfVectorizer</li>
								</ul>
							Snapshot of data after cleaning : <br/><br/>
							<img src="images/Clean.png" alt="Clean" width="400" height="400"/>
							<br/>
							Script used for data procession and cleaning : <a href="https://colab.research.google.com/drive/16FyHdkw5OkLxCgIExuys4CiYoT7Okxwz?usp=sharing" target="_blank">Data_Cleaning.ipynb</a>	
							</p>
					
						</div>
					</div>
				</section>
				<section>
					<div class="content">
						<div class="inner">
							<h2>Code</h2>
							<p>
							Script used for Topic Modeling : <a href="https://colab.research.google.com/drive/1H-0osBdNtbseU9DpU9V4_bN5tRgMH55v?usp=sharing" target="_blank">TopicModeling.ipynb</a>
							</p>
					
						</div>
					</div>
				</section>

				<section>
					<div class="content">
						<div class="inner">
							<h2>Results</h2>
							<p>The approach to finding the optimal number of topics is to build many LDA models with different values of number of topics (k) and pick the one that gives the highest coherence value.
								Choosing a ‘k’ that marks the end of a rapid growth of topic coherence usually offers meaningful and interpretable topics. Picking an even higher value can sometimes provide more granular sub-topics.
								If you see the same keywords being repeated in multiple topics, it’s probably a sign that the ‘k’ is too large. Keeping this in mind and as show in the coherence plot below :<br/>
								<img src="images/Coherence_LDA.png" alt="Clean" width="400" height="400"/>
								<br/>
								We can see that the optimal number of topics here is 20. Hence,the optimal LDA model is built with 20 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage to the topic.
								You can see the keywords for each topic and the weightage(importance) of each keyword below : 
								<br/><br/>
								(0,
  '0.072*"also" + 0.054*"much" + 0.039*"lot" + 0.036*"thing" + 0.035*"ask" + '
  '0.033*"ads" + 0.033*"question" + 0.028*"money" + 0.028*"internet" + '
  '0.027*"asked"'),<br/><br/>
 (1,
  '0.141*"google" + 0.096*"chatgpt" + 0.048*"search" + 0.041*"even" + '
  '0.033*"one" + 0.030*"use" + 0.027*"cannot" + 0.027*"better" + '
  '0.024*"answer" + 0.023*"need"'),<br/><br/>
 (2,
  '0.040*"everything" + 0.024*"brain" + 0.024*"coding" + 0.023*"start" + '
  '0.023*"everyone" + 0.020*"try" + 0.019*"whole" + 0.017*"almost" + '
  '0.014*"generation" + 0.013*"stupid"'),<br/><br/>
 (3,
  '0.134*"people" + 0.052*"years" + 0.048*"really" + 0.042*"actually" + '
  '0.036*"without" + 0.028*"probably" + 0.028*"making" + 0.027*"yes" + '
  '0.026*"maybe" + 0.021*"bad"'),<br/><br/>
 (4,
  '0.114*"know" + 0.046*"sure" + 0.034*"pretty" + 0.030*"big" + 0.029*"us" + '
  '0.027*"free" + 0.026*"example" + 0.025*"great" + 0.025*"possible" + '
  '0.024*"tool"'),<br/><br/>
 (5,
  '0.082*"human" + 0.060*"still" + 0.024*"content" + 0.021*"said" + '
  '0.020*"superman" + 0.020*"writing" + 0.018*"tools" + 0.017*"creative" + '
  '0.017*"humans" + 0.014*"needs"'),<br/><br/>
 (6,
  '0.090*"things" + 0.068*"want" + 0.054*"say" + 0.031*"stuff" + 0.022*"lol" + '
  '0.022*"correct" + 0.021*"basically" + 0.020*"system" + 0.016*"stop" + '
  '0.015*"care"'),<br/><br/>
 (7,
  '0.086*"right" + 0.032*"ever" + 0.028*"product" + 0.025*"products" + '
  '0.021*"least" + 0.020*"remember" + 0.017*"build" + 0.016*"post" + '
  '0.014*"gmail" + 0.013*"edit"'),<br/><br/>
 (8,
  '0.071*"well" + 0.037*"technology" + 0.035*"far" + 0.034*"mean" + '
  '0.032*"help" + 0.027*"similar" + 0.023*"page" + 0.019*"lamda" + '
  '0.018*"advice" + 0.017*"wanted"'),<br/><br/>
 (9,
  '0.058*"never" + 0.050*"gpt" + 0.035*"chat" + 0.033*"problem" + '
  '0.029*"might" + 0.024*"models" + 0.024*"company" + 0.022*"anyone" + '
  '0.021*"tell" + 0.021*"mind"'),<br/><br/>
 (10,
  '0.032*"let" + 0.030*"always" + 0.027*"real" + 0.025*"market" + 0.021*"away" '
  '+ 0.018*"clone" + 0.017*"wait" + 0.017*"simple" + 0.016*"talking" + '
  '0.016*"invented"'),<br/><br/>
 (11,
  '0.126*"ai" + 0.049*"time" + 0.043*"something" + 0.032*"work" + '
  '0.032*"already" + 0.030*"way" + 0.021*"many" + 0.020*"take" + 0.018*"room" '
  '+ 0.015*"job"'),<br/><br/>
 (12,
  '0.051*"data" + 0.044*"model" + 0.039*"code" + 0.037*"language" + '
  '0.037*"using" + 0.034*"write" + 0.030*"understand" + 0.027*"someone" + '
  '0.023*"based" + 0.022*"text"'),<br/><br/>
 (13,
  '0.091*"like" + 0.088*"would" + 0.023*"anything" + 0.019*"wrong" + '
  '0.019*"find" + 0.016*"point" + 0.014*"though" + 0.012*"saying" + '
  '0.012*"two" + 0.012*"around"'),<br/><br/>
 (14,
  '0.090*"get" + 0.086*"make" + 0.045*"go" + 0.029*"business" + '
  '0.024*"nothing" + 0.023*"means" + 0.020*"put" + 0.020*"run" + '
  '0.019*"machine" + 0.018*"exactly"'),<br/><br/>
 (15,
  '0.064*"able" + 0.058*"every" + 0.034*"best" + 0.031*"getting" + 0.024*"end" '
  '+ 0.023*"likely" + 0.022*"done" + 0.022*"version" + 0.021*"lead" + '
  '0.021*"doubt"'),<br/><br/>
 (16,
  '0.085*"art" + 0.058*"results" + 0.038*"made" + 0.027*"useful" + '
  '0.021*"software" + 0.019*"public" + 0.019*"test" + 0.015*"accuracy" + '
  '0.015*"train" + 0.014*"illegal"'),<br/><br/>
 (17,
  '0.099*"think" + 0.080*"could" + 0.070*"going" + 0.057*"see" + '
  '0.043*"different" + 0.041*"microsoft" + 0.027*"openai" + 0.025*"day" + '
  '0.014*"huge" + 0.013*"comes"'),<br/><br/>
 (18,
  '0.049*"questions" + 0.042*"instead" + 0.033*"makes" + 0.030*"knowledge" + '
  '0.025*"learn" + 0.022*"game" + 0.022*"hard" + 0.021*"ability" + '
  '0.017*"asking" + 0.017*"user"'),<br/><br/>
 (19,
  '0.102*"good" + 0.039*"read" + 0.035*"got" + 0.029*"part" + 0.023*"believe" '
  '+ 0.022*"result" + 0.021*"scale" + 0.018*"interview" + 0.017*"general" + '
  '0.017*"last"')
								<br/><br/>
								<h5>How to interpret this ? </h5>
								<br/>
								Topic 0 is a represented as '0.072*"also" + 0.054*"much" + 0.039*"lot" + 0.036*"thing" + 0.035*"ask" + ' '0.033*"ads" + 0.033*"question" + 0.028*"money" + 0.028*"internet" + ' '0.027*"asked"'.

								It means the top 10 keywords that contribute to this topic are: ‘also’, ‘much, ‘thing’.. and so on and the weight of ‘also’ on topic 0 is 0.072.
								
								The weights reflect how important a keyword is to that topic.
								
								Looking at these keywords, we can probably guess that this topic could be about Chat GPT making money via Ads in the future.<br/>
								<br/><img src="images/LDA.png" alt="Clean" width="800" height="600"/>
<br/><br/>
								Each bubble on the left-hand side plot represents a topic. The larger the bubble, the more prevalent is that topic.

A good topic model will have fairly big, non-overlapping bubbles scattered throughout the chart instead of being clustered in one quadrant.

A model with too many topics, will typically have many overlaps, small sized bubbles clustered in one region of the chart.

We have successfully built a good looking topic model.

							</p>
					
						</div>
					</div>
				</section>

				<section>
					<div class="content">
						<div class="inner">
							<h2>Conclusion</h2>
							<p>
								To conclude, we started with understanding what topic modeling can do. Then, we built a basic topic model using Gensim’s LDA and visualized the topics using pyLDAvis.
								Finally, we saw how to find the optimal number of topics using coherence scores and how we can come to a logical understanding of how to choose the optimal model.
							</p>
					
						</div>
					</div>
				</section>
			</section>

			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper style1-alt">
				<div class="inner">
					<ul class="menu">
						<li>&copy; All rights reserved.</li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>